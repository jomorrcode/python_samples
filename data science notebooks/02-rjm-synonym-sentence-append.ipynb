{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append sentence with replacement synonym\n",
    "\n",
    "This notebook loads a dataset, tokenizes it, and based on criteria from the user replaces candidate tokens in the corpus with a token indicating it is the original token/sentence and, based upon some probability, appends an identical sentence to the document with the token replaced with a token that indicates it is an appended token/sentence.  This should assist in validating word embeddings if a sentence and it's appended synonym sentence show up close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Quick cell to make jupyter notebook use the full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/john/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/john/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import vectorizers\n",
    "import textmap.tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "import string\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "from src.data import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.load('reddit_comment_tree_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(dataset['data']['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 2s, sys: 5.2 s, total: 5min 7s\n",
      "Wall time: 5min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus = textmap.tokenizers.NLTKTweetTokenizer(tokenize_by='sentence').fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open(\"/home/john/Code/NLP_Utilities/corpus\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token replacement function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_sentence_append(\n",
    "        corpus,\n",
    "        ignored_tokens=None,\n",
    "        excluded_token_regex=None,\n",
    "        min_frequency=None,\n",
    "        max_frequency=None,\n",
    "        min_occurrences=None,\n",
    "        max_occurrences=None,\n",
    "        min_document_frequency=None,\n",
    "        max_document_frequency=None,\n",
    "        min_document_occurrences=None,\n",
    "        max_document_occurrences=None,\n",
    "        num_candidates=25,\n",
    "        tokens_to_replace=None,\n",
    "        replace_probability=0.3):\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    :param corpus: a tuple of tuples of tokenized sentences\n",
    "\n",
    "    :param ignored_tokens: a set of tokens to prune from token dictionary\n",
    "\n",
    "    :param excluded_token_regex: a regex pattern to identify tokens to prune from token dictionary\n",
    "\n",
    "    :param min_frequency: float - The minimum frequency of occurrence allowed for tokens. Tokens that occur\n",
    "        less frequently than this will be pruned.\n",
    "\n",
    "    :param max_frequency: float - The maximum frequency of occurrence allowed for tokens. Tokens that occur\n",
    "        more frequently than this will be pruned.\n",
    "\n",
    "    :param min_occurrences: int - A constraint on the minimum number of occurrences for a token to be considered\n",
    "        valid. If None then no constraint will be applied.\n",
    "\n",
    "    :param max_occurrences: int - A constraint on the maximum number of occurrences for a token to be considered\n",
    "        valid. If None then no constraint will be applied.\n",
    "\n",
    "    :param min_document_frequency: int - A constraint on the minimum frequency of documents with occurrences for a\n",
    "        token to be considered valid. If None then no constraint will be applied.\n",
    "\n",
    "    :param max_document_frequency: int - A constraint on the maximum frequency of documents with occurrences for a\n",
    "        token to be considered valid. If None then no constraint will be applied.\n",
    "\n",
    "    :param min_document_occurrences: int - A constraint on the minimum number of documents with occurrences for a\n",
    "        token to be considered valid. If None then no constraint will be applied.\n",
    "\n",
    "    :param max_document_occurrences: int - A constraint on the maximum number of documents with occurrences for a\n",
    "        token to be considered valid. If None then no constraint will be applied.\n",
    "\n",
    "\n",
    "    :param num_candidates: int - The number of candidate tokens to be replaced with synonyms\n",
    "\n",
    "    :param tokens_to_replace: list - A list of tokens to be replaced with synonyms.  If None, the other parameters\n",
    "        will be used to select tokens to replace\n",
    "\n",
    "    :param replace_probability: float - the probability a new synonym sentence will be added to the corpus\n",
    "\n",
    "    :return:  a tuple of tuples of tokenized sentences containing new synonym in place of original tokens and\n",
    "        a list of the words that were replaced\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    #check if tokens to be replaced are supplied, if not, choose tokens depending on parameters from user\n",
    "    if not tokens_to_replace:\n",
    "\n",
    "        # flatten tuple of tuples and get token dictionary\n",
    "        token_dict, token_freq, n_tokens = vectorizers._vectorizers.construct_token_dictionary_and_frequency(\n",
    "            vectorizers.utils.flatten(corpus))\n",
    "\n",
    "        # prune token dictionary depending on parameters supplied by user\n",
    "        # returns a dictionary of candidate tokens for replacement\n",
    "        candidate_dict, candidate_freq = vectorizers._vectorizers.prune_token_dictionary(\n",
    "            token_dict,\n",
    "            token_freq,\n",
    "            ignored_tokens=ignored_tokens,\n",
    "            excluded_token_regex=excluded_token_regex,\n",
    "            min_frequency=min_frequency,\n",
    "            max_frequency=max_frequency,\n",
    "            min_occurrences=min_occurrences,\n",
    "            max_occurrences=max_occurrences,\n",
    "            min_document_frequency=min_document_frequency,\n",
    "            max_document_frequency=max_document_frequency,\n",
    "            min_document_occurrences=min_document_occurrences,\n",
    "            max_document_occurrences=max_document_occurrences,\n",
    "            total_tokens=n_tokens,\n",
    "            total_documents=len(corpus),\n",
    "        )\n",
    "\n",
    "        # take a random sample of tokens from the candidate dictionary\n",
    "        tokens_to_replace = random.sample(list(candidate_dict.keys()), num_candidates)\n",
    "\n",
    "\n",
    "    print(\"Tokens for replacement:\")\n",
    "    print(tokens_to_replace)\n",
    "\n",
    "    new_corpus = []\n",
    "\n",
    "    for sent in corpus:\n",
    "        word_changed = False\n",
    "        sent = list(sent)\n",
    "\n",
    "        # check each token by index and create a deep copy with the changed word at that index and add new sentence to new corpus\n",
    "        for idx,token in enumerate(sent):\n",
    "            if token in tokens_to_replace:\n",
    "                new_sent=copy.deepcopy(sent)\n",
    "                new_sent[idx] = f\"{token}_$$0\"\n",
    "                new_corpus.append(new_sent)\n",
    "                word_changed = True\n",
    "\n",
    "                # depending on probability, add another copy of the new sentence with the second replacement synonym\n",
    "                if random.random() <= replace_probability:\n",
    "                    added_sent=copy.deepcopy(sent)\n",
    "                    added_sent[idx] = f\"{token}_$$1\"\n",
    "                    new_corpus.append(added_sent)\n",
    "\n",
    "\n",
    "        # if no words were changed, just add the original sentence to the new corpus\n",
    "        if not word_changed:\n",
    "            new_corpus.append(sent)\n",
    "            \n",
    "        \n",
    "    # change dataset back to tuple of tuples before returning\n",
    "    new_corpus_tuple = tuple(tuple(sent) for sent in new_corpus)\n",
    "\n",
    "\n",
    "    return new_corpus_tuple, tokens_to_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for replacement:\n",
      "['distinction', 'documented', 'protected', 'pm', 'muslims', 'incredible', 'supposedly', 'kick', 'targeted', 'unnecessary', 'breath', 'careful', 'bully', 'coward', 'rejected', 'blackout', 'forcing', 'mail', 'rolls', 'requirements']\n",
      "CPU times: user 42.3 s, sys: 330 ms, total: 42.6 s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "new_corpus, replaced_tokens = synonym_sentence_append(\n",
    "        corpus,\n",
    "        min_occurrences=1000,max_occurrences=2000,\n",
    "        ignored_tokens={'deleted','removed'} | set(stopwords.words('english')) | set(string.punctuation)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('official',\n",
       "  'meetings',\n",
       "  'with',\n",
       "  'adversaries',\n",
       "  'by',\n",
       "  'presidents',\n",
       "  ',',\n",
       "  'secretary',\n",
       "  'of',\n",
       "  'states',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'congress',\n",
       "  'are',\n",
       "  'not',\n",
       "  'secret',\n",
       "  '...',\n",
       "  'unless',\n",
       "  'of',\n",
       "  'course',\n",
       "  'you',\n",
       "  'are',\n",
       "  'donald',\n",
       "  'trump_$$0',\n",
       "  '...',\n",
       "  'or',\n",
       "  'tulsi',\n",
       "  'gabbard',\n",
       "  '.'),\n",
       " ('official',\n",
       "  'meetings',\n",
       "  'with',\n",
       "  'adversaries',\n",
       "  'by',\n",
       "  'presidents',\n",
       "  ',',\n",
       "  'secretary',\n",
       "  'of',\n",
       "  'states',\n",
       "  'and',\n",
       "  'members',\n",
       "  'of',\n",
       "  'congress',\n",
       "  'are',\n",
       "  'not',\n",
       "  'secret',\n",
       "  '...',\n",
       "  'unless',\n",
       "  'of',\n",
       "  'course',\n",
       "  'you',\n",
       "  'are',\n",
       "  'donald',\n",
       "  'trump_$$1',\n",
       "  '...',\n",
       "  'or',\n",
       "  'tulsi',\n",
       "  'gabbard',\n",
       "  '.'),\n",
       " ('https://www.nbcnews.com/politics/elections/bernie-sanders-camp-fix-was-against-clinton-n817501',\n",
       "  'http://nymag.com/intelligencer/2019/06/bernie-sanders-2016-rigged-wont-pledge-support-winner.html',\n",
       "  'i',\n",
       "  'am',\n",
       "  'sure',\n",
       "  \"i'll\",\n",
       "  'find',\n",
       "  'some',\n",
       "  'more',\n",
       "  'sources',\n",
       "  'if',\n",
       "  'you',\n",
       "  'insist',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she_$$0',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she_$$1',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq_$$0',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq_$$1',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she_$$0',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('so',\n",
       "  'she',\n",
       "  'volunteered',\n",
       "  'to',\n",
       "  'serve',\n",
       "  'in',\n",
       "  'iraq',\n",
       "  ',',\n",
       "  'a',\n",
       "  'war',\n",
       "  'she_$$1',\n",
       "  \"didn't\",\n",
       "  'agree',\n",
       "  'with',\n",
       "  '.'),\n",
       " ('if',\n",
       "  'you',\n",
       "  'think',\n",
       "  'about',\n",
       "  \"epstein's\",\n",
       "  'mysterious',\n",
       "  'death',\n",
       "  ',',\n",
       "  'yeah',\n",
       "  'barr',\n",
       "  'will',\n",
       "  'do',\n",
       "  'anything',\n",
       "  'to',\n",
       "  'protect',\n",
       "  'trump_$$0',\n",
       "  'or',\n",
       "  'at',\n",
       "  'least',\n",
       "  'facilitate',\n",
       "  'another',\n",
       "  'gop',\n",
       "  'term',\n",
       "  '.'))"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4284273"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4293246"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,sent in enumerate(new_corpus):\n",
    "    for token in sent:\n",
    "        if \"_$$1\" in token:\n",
    "            print(new_corpus[idx])\n",
    "            print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('>', '>', '\"', \"it's\", 'been', 'well', 'documented_$$0', 'in', 'the', 'financial', 'times', ',', 'in', 'politico', ',', 'in', 'the', 'economist', ',', 'in', 'the', 'washington', 'examiner', ',', 'even', 'on', 'cbs', ',', 'that', 'the', 'prime', 'minister', 'of', 'ukraine', ',', 'the', 'interior', 'minister', ',', 'the', 'ukrainian', 'ambassador', 'to', 'the', 'united', 'states', ',', 'the', 'head', 'of', 'the', 'ukrainian', 'anti-corruption', 'league', ',', 'all', 'meddled', 'in', 'the', 'election', 'on', 'social', 'media', 'and', 'otherwise', ',', '\"', 'he', 'said', '.')\n",
      "=============================================\n",
      "('[', 'x', ']', 'rolls_$$0', 'back', 'protective', 'regulations', 'for', 'the', 'american', 'people', '.')\n",
      "=============================================\n",
      "('[', 'x', ']', 'rolls_$$1', 'back', 'protective', 'regulations', 'for', 'the', 'american', 'people', '.')\n",
      "=============================================\n",
      "('because', 'people', 'buy', 'a', 'huge', 'amount', 'of', 'stuff', 'right', 'before', 'tariffs', 'kick_$$0', 'in', ',', \"that's\", 'the', 'boom', ',', 'the', 'decline', 'happens', 'after', 'when', 'people', 'buy', 'minimum', 'amounts', '.')\n",
      "=============================================\n",
      "('you', 'are', 'technically', 'correct', 'that', 'they', \"aren't\", 'compeled', 'to', 'do', 'so', 'and', 'in', 'an', 'abstract', 'sense', 'we', 'would', 'have', 'to', 'be', 'careful_$$0', '.')\n",
      "=============================================\n",
      "('if', 'anyone', 'is', 'being', '\"', 'targeted_$$0', '\"', 'it', 'is', 'the', 'people', 'who', 'are', 'implied', 'to', 'have', 'lesser', 'ability', 'to', 'make', 'informed', 'decisions', '.')\n",
      "=============================================\n",
      "('inconveniently', 'his', '*', 'general', 'ignorance', '*', 'of', 'how', 'the', 'government', 'works', 'is', 'quite', 'well', 'documented_$$0', '.')\n",
      "=============================================\n",
      "('as', 'for', 'constitutional', 'requirements_$$0', ',', 'there', \"isn't\", 'one', 'to', 'impeach', 'regardless', 'of', 'removal', '.')\n",
      "=============================================\n",
      "('as', 'for', 'constitutional', 'requirements_$$1', ',', 'there', \"isn't\", 'one', 'to', 'impeach', 'regardless', 'of', 'removal', '.')\n",
      "=============================================\n",
      "('if', \"that's\", 'the', 'case', ',', 'you', 'should', 'be', 'very', 'careful_$$0', 'around', 'all', 'ideology', '.')\n",
      "=============================================\n",
      "('if', \"that's\", 'the', 'case', ',', 'you', 'should', 'be', 'very', 'careful_$$1', 'around', 'all', 'ideology', '.')\n",
      "=============================================\n",
      "('applause', 'is', 'reserved', 'for', 'those', 'who', 'are', 'actually', 'ready', 'to', 'pay', 'the', 'price', 'of', 'not', 'forcing_$$0', 'through', 'their', 'policy', 'at', 'any', 'cost', ',', 'yet', 'flake', 'undermined', 'the', 'constitution', 'when', 'obama', 'nominated', 'garland', 'and', 'was', 'unwilling', 'to', 'forgo', 'any', 'republican', 'values', 'in', 'favor', 'of', 'american', 'values', 'for', 'the', 'remainder', 'of', 'his', 'senate', 'term', '.')\n",
      "=============================================\n",
      "(\"that's\", 'lovely', ',', 'but', 'i', \"wasn't\", '*', 'advocating', '*', 'for', 'anything', 'past', ',', 'perhaps', ',', 'getting', 'onto', 'the', 'damn', 'requirements_$$0', 'for', '*', 'doctors', 'to', 'lie', '*', 'before', 'jumping', 'the', 'pond', 'to', 'europe', 'for', 'examples', 'of', 'mostly', 'free', 'but', 'sometimes', 'strategically', 'restricted', 'speech', '.')\n",
      "=============================================\n",
      "('it', 'will', 'need', 'real', 'organization', 'for', 'that', ',', 'not', 'indivisible', 'hyping', 'nation-wide', 'protests', 'in', 'one', 'breath_$$0', '(', 'taking', 'out', 'all', 'the', 'oxygen', ')', 'and', 'that', 'you', 'should', 'set', 'it', 'up', 'to', 'set', 'up', '(', 'under', 'their', 'umbrella', 'of', 'course', ')', 'if', 'there', \"isn't\", 'anything', 'going', 'on', 'near', 'you', '.')\n",
      "=============================================\n",
      "('it', 'will', 'need', 'real', 'organization', 'for', 'that', ',', 'not', 'indivisible', 'hyping', 'nation-wide', 'protests', 'in', 'one', 'breath_$$1', '(', 'taking', 'out', 'all', 'the', 'oxygen', ')', 'and', 'that', 'you', 'should', 'set', 'it', 'up', 'to', 'set', 'up', '(', 'under', 'their', 'umbrella', 'of', 'course', ')', 'if', 'there', \"isn't\", 'anything', 'going', 'on', 'near', 'you', '.')\n",
      "=============================================\n",
      "('adjusting', 'weights', ',', 'which', 'search', 'engines', 'that', 'are', 'not', '*', 'completely', '*', 'flooded', 'with', 'spam', 'do', 'all', 'the', 'time', 'is', 'not', 'the', 'same', 'as', 'telling', 'a', 'person', 'to', 'shut', 'the', 'fuck', 'up', ',', 'or', 'even', 'forcing_$$0', 'doctors', 'to', 'lie', 'to', 'their', 'patients', '.')\n",
      "=============================================\n",
      "('anyway', ',', 'while', 'the', 'us', 'gets', 'away', 'with', 'forcing_$$0', 'doctors', 'to', 'lie', 'to', 'patients', 'you', 'will', 'have', 'to', 'step', 'down', 'from', 'your', 'high', 'and', 'mighty', 'free', 'speech', 'podium', 'and', 'learn', 'some', 'shit', 'about', 'how', 'the', 'sausage', 'that', 'is', 'your', 'search', 'results', 'gets', 'made', '.')\n",
      "=============================================\n",
      "('anyway', ',', 'while', 'the', 'us', 'gets', 'away', 'with', 'forcing_$$1', 'doctors', 'to', 'lie', 'to', 'patients', 'you', 'will', 'have', 'to', 'step', 'down', 'from', 'your', 'high', 'and', 'mighty', 'free', 'speech', 'podium', 'and', 'learn', 'some', 'shit', 'about', 'how', 'the', 'sausage', 'that', 'is', 'your', 'search', 'results', 'gets', 'made', '.')\n",
      "=============================================\n",
      "('states', 'and', 'municipalities', ',', 'on', 'the', 'other', 'hand', ',', 'can', 'and', 'do', 'make', 'these', 'requirements_$$0', '.')\n",
      "=============================================\n",
      "('states', 'and', 'municipalities', ',', 'on', 'the', 'other', 'hand', ',', 'can', 'and', 'do', 'make', 'these', 'requirements_$$1', '.')\n",
      "=============================================\n",
      "('and', 'in', 'the', 'next', 'breath_$$0', 'want', 'to', 'violate', 'the', 'constitution', 'by', 'not', 'sending', 'the', 'articles', 'to', 'the', 'senate', 'as', 'required', 'by', 'the', 'constitution', ';', 'and', 'then', 'use', 'it', 'as', 'a', 'bargaining', 'tool', 'to', 'be', 'able', 'to', 'also', 'make', 'rules', 'in', 'the', 'senate', 'and', 'require', 'the', 'senate', 'to', 'investigate', 'further', '(', 'both', 'of', 'which', 'are', 'against', 'the', 'constitution', ')', ',', 'so', 'that', 'the', 'rules', 'and', 'processes', 'they', 'refused', 'to', 'give', 'republicans', 'in', 'the', 'house', 'are', 'given', 'to', 'them', 'in', 'the', 'senate', '.')\n",
      "=============================================\n",
      "('just', 'compare', 'our', 'voting', 'requirements_$$0', 'to', 'everyone', 'else', '.')\n",
      "=============================================\n",
      "('that', 'was', 'the', 'line', 'all', 'over', 'conservative', 'media', ',', 'too', ',', 'and', 'on', 'r', '\\\\', '/', 'the_donald', ',', 'who', 'were', 'waiting', 'for', 'it', 'with', 'bated', 'breath_$$0', '.')\n",
      "=============================================\n",
      "('please', 'explain', 'how', 'the', 'british', 'pm_$$0', 'becomes', 'pm', 'after', 'an', 'election', 'like', 'the', 'one', 'scheduled', 'for', 'dec', '12', '.', \"i'm\", 'listening', '...', '>', 'dave', 'chappelle', 'episode', 'of', 'saturday', 'night', 'live', 'https://www.youtube.com/watch?v=shg0ezlivgc', 'but', 'then', 'where', 'will', 'they', 'go', '?')\n",
      "=============================================\n",
      "('please', 'explain', 'how', 'the', 'british', 'pm', 'becomes', 'pm_$$0', 'after', 'an', 'election', 'like', 'the', 'one', 'scheduled', 'for', 'dec', '12', '.', \"i'm\", 'listening', '...', '>', 'dave', 'chappelle', 'episode', 'of', 'saturday', 'night', 'live', 'https://www.youtube.com/watch?v=shg0ezlivgc', 'but', 'then', 'where', 'will', 'they', 'go', '?')\n",
      "=============================================\n",
      "('https://philosiblog.com/2012/03/07/it-is-the-mark-of-an-educated-mind-to-be-able-to-entertain-a-thought-without-accepting-it/', 'smashed', 'between', 'its', 'pages', 'gasping', 'for', 'breath_$$0', 'as', 'it', 'relentlessly', 'suffocates', 'you', '.')\n",
      "=============================================\n",
      "('https://philosiblog.com/2012/03/07/it-is-the-mark-of-an-educated-mind-to-be-able-to-entertain-a-thought-without-accepting-it/', 'smashed', 'between', 'its', 'pages', 'gasping', 'for', 'breath_$$1', 'as', 'it', 'relentlessly', 'suffocates', 'you', '.')\n",
      "=============================================\n",
      "('i', 'know', ',', 'but', 'the', 'public', 'voting', 'for', 'his', 'party', 'with', 'the', 'knowledge', 'the', 'party', 'will', 'try', 'to', 'make', 'him', 'pm_$$0', '.')\n",
      "=============================================\n",
      "('this', 'is', 'the', 'procedure', 'that', 'should', 'be', 'protected_$$0', 'and', 'this', 'is', 'the', 'result', 'of', 'separate', 'but', 'equal', 'branches', '.')\n",
      "=============================================\n",
      "('this', 'is', 'the', 'procedure', 'that', 'should', 'be', 'protected_$$1', 'and', 'this', 'is', 'the', 'result', 'of', 'separate', 'but', 'equal', 'branches', '.')\n",
      "=============================================\n",
      "('implying', 'that', 'forcing_$$0', 'people', 'to', 'pay', 'for', 'things', 'they', 'don', '’', 't', 'use', 'and', 'don', '’', 't', 'like', 'is', 'a', 'net', 'positive', '...', 'implying', 'that', 'forcing', 'everyone', 'to', 'fund', 'political', 'campaigns', 'that', 'they', 'do', 'not', 'support', 'is', 'a', 'net', 'positive', '...')\n",
      "=============================================\n",
      "('implying', 'that', 'forcing_$$1', 'people', 'to', 'pay', 'for', 'things', 'they', 'don', '’', 't', 'use', 'and', 'don', '’', 't', 'like', 'is', 'a', 'net', 'positive', '...', 'implying', 'that', 'forcing', 'everyone', 'to', 'fund', 'political', 'campaigns', 'that', 'they', 'do', 'not', 'support', 'is', 'a', 'net', 'positive', '...')\n",
      "=============================================\n",
      "('implying', 'that', 'forcing', 'people', 'to', 'pay', 'for', 'things', 'they', 'don', '’', 't', 'use', 'and', 'don', '’', 't', 'like', 'is', 'a', 'net', 'positive', '...', 'implying', 'that', 'forcing_$$0', 'everyone', 'to', 'fund', 'political', 'campaigns', 'that', 'they', 'do', 'not', 'support', 'is', 'a', 'net', 'positive', '...')\n",
      "=============================================\n",
      "('feldman', 'should', 'explain', 'how', 'he', 'can', 'advocate', 'criteria', 'for', 'impeachment', 'that', 'were', 'rejected_$$0', 'by', 'the', 'framers', '.')\n",
      "=============================================\n",
      "('how', 'can', '“', 'high', 'crimes', 'and', 'misdemeanors', '“', 'be', 'interpreted', 'to', 'mean', 'precisely', 'what', 'the', 'framers', 'rejected_$$0', ':', '“', 'maladministration', '”', 'and', 'other', 'open', 'ended', 'criteria', '.')\n",
      "=============================================\n",
      "('how', 'can', '“', 'high', 'crimes', 'and', 'misdemeanors', '“', 'be', 'interpreted', 'to', 'mean', 'precisely', 'what', 'the', 'framers', 'rejected_$$1', ':', '“', 'maladministration', '”', 'and', 'other', 'open', 'ended', 'criteria', '.')\n",
      "=============================================\n",
      "('it', 'is', 'almost', 'like', 'forcing_$$0', 'all', 'big', 'business', 'into', 'a', 'concentrated', 'area', 'with', 'tax', 'incentives', 'will', 'inflate', 'housing', 'prices', 'as', 'there', \"isn't\", 'enough', 'housing', 'for', 'the', 'workers', '.')\n",
      "=============================================\n",
      "('it', 'only', 'further', 'proves', 'my', 'point', 'and', 'in', 'the', 'same', 'breath_$$0', 'proves', 'you', 'have', 'zero', 'leg', 'to', 'stand', 'on', '.')\n",
      "=============================================\n",
      "('it', 'only', 'further', 'proves', 'my', 'point', 'and', 'in', 'the', 'same', 'breath_$$1', 'proves', 'you', 'have', 'zero', 'leg', 'to', 'stand', 'on', '.')\n",
      "=============================================\n",
      "('they', 'can', 'just', 'kick_$$0', 'you', 'out', '?')\n",
      "=============================================\n",
      "('oh', 'well', ',', \"i'm\", 'sure', 'there', 'will', 'be', 'many', 'more', 'unnecessary_$$0', 'deaths', 'and', 'public', 'spendature', '.')\n",
      "=============================================\n",
      "('oh', 'well', ',', \"i'm\", 'sure', 'there', 'will', 'be', 'many', 'more', 'unnecessary_$$1', 'deaths', 'and', 'public', 'spendature', '.')\n",
      "=============================================\n",
      "('it', 'makes', 'sense', 'he', 'would', 'be', 'careful_$$0', 'to', 'deny', 'any', 'wrongdoing', 'at', 'that', 'point', '.')\n",
      "=============================================\n",
      "('it', 'makes', 'sense', 'he', 'would', 'be', 'careful_$$1', 'to', 'deny', 'any', 'wrongdoing', 'at', 'that', 'point', '.')\n",
      "=============================================\n",
      "('let', 'everyone', 'live', 'side', 'by', 'side', 'as', 'neighbors', 'without', 'forcing_$$0', 'your', 'beliefs', 'on', 'others', ',', 'and', 'have', 'our', 'government', 'follow', 'the', 'constitution', '.')\n",
      "=============================================\n",
      "(\"there's\", 'documented_$$0', 'evidence', '.')\n",
      "=============================================\n",
      "('the', 'trump', 'organization', 'is', 'still', 'running', 'and', 'by', 'forcing_$$0', 'the', 'us', 'government', 'to', 'hold', 'meetings', 'at', 'his', 'personal', 'businesses', '(', 'and', 'charging', 'taxpayers', 'for', 'it', ')', \"he's\", 'profiting', 'from', 'his', 'office', '.')\n",
      "=============================================\n",
      "('trump', 'violated', 'this', 'law', 'by', 'forcing_$$0', 'staff', 'to', 'stay', 'at', 'his', 'own', 'hotels', 'during', 'trips', 'meetings', 'and', 'then', 'forcing', 'the', 'government', 'to', 'pay', 'for', 'it', '.')\n",
      "=============================================\n",
      "('trump', 'violated', 'this', 'law', 'by', 'forcing', 'staff', 'to', 'stay', 'at', 'his', 'own', 'hotels', 'during', 'trips', 'meetings', 'and', 'then', 'forcing_$$0', 'the', 'government', 'to', 'pay', 'for', 'it', '.')\n",
      "=============================================\n",
      "('if', 'they', 'are', 'going', 'to', 'argue', 'the', 'information', 'is', 'classified', 'or', 'protected_$$0', ',', 'they', 'need', 'to', 'argue', 'that', 'on', 'a', 'question-by-question', 'basis', '.')\n",
      "=============================================\n",
      "('if', 'they', 'are', 'going', 'to', 'argue', 'the', 'information', 'is', 'classified', 'or', 'protected_$$1', ',', 'they', 'need', 'to', 'argue', 'that', 'on', 'a', 'question-by-question', 'basis', '.')\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "for sent in new_corpus[100:5000]:\n",
    "    for token in sent:\n",
    "        if \"_$$\" in token:\n",
    "            print(sent)\n",
    "            print(\"=============================================\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corpus = list(list(sent) for sent in tokens[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so',\n",
       " 'she',\n",
       " 'volunteered',\n",
       " 'to',\n",
       " 'serve',\n",
       " 'in',\n",
       " 'iraq',\n",
       " ',',\n",
       " 'a',\n",
       " 'war',\n",
       " 'she',\n",
       " \"didn't\",\n",
       " 'agree',\n",
       " 'with',\n",
       " '.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_replace = ['trump', 'secret', 'iraq', 'she']\n",
    "prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['official', 'meetings', 'with', 'adversaries', 'by', 'presidents', ',', 'secretary', 'of', 'states', 'and', 'members', 'of', 'congress', 'are', 'not', 'secret_$$0', '...', 'unless', 'of', 'course', 'you', 'are', 'donald', 'trump', '...', 'or', 'tulsi', 'gabbard', '.']\n",
      "['official', 'meetings', 'with', 'adversaries', 'by', 'presidents', ',', 'secretary', 'of', 'states', 'and', 'members', 'of', 'congress', 'are', 'not', 'secret_$$1', '...', 'unless', 'of', 'course', 'you', 'are', 'donald', 'trump', '...', 'or', 'tulsi', 'gabbard', '.']\n",
      "\n",
      "['official', 'meetings', 'with', 'adversaries', 'by', 'presidents', ',', 'secretary', 'of', 'states', 'and', 'members', 'of', 'congress', 'are', 'not', 'secret', '...', 'unless', 'of', 'course', 'you', 'are', 'donald', 'trump_$$0', '...', 'or', 'tulsi', 'gabbard', '.']\n",
      "['official', 'meetings', 'with', 'adversaries', 'by', 'presidents', ',', 'secretary', 'of', 'states', 'and', 'members', 'of', 'congress', 'are', 'not', 'secret', '...', 'unless', 'of', 'course', 'you', 'are', 'donald', 'trump_$$1', '...', 'or', 'tulsi', 'gabbard', '.']\n",
      "\n",
      "['https://www.nbcnews.com/politics/elections/bernie-sanders-camp-fix-was-against-clinton-n817501', 'http://nymag.com/intelligencer/2019/06/bernie-sanders-2016-rigged-wont-pledge-support-winner.html', 'i', 'am', 'sure', \"i'll\", 'find', 'some', 'more', 'sources', 'if', 'you', 'insist', '.']\n",
      "\n",
      "['so', 'she_$$0', 'volunteered', 'to', 'serve', 'in', 'iraq', ',', 'a', 'war', 'she', \"didn't\", 'agree', 'with', '.']\n",
      "['so', 'she_$$1', 'volunteered', 'to', 'serve', 'in', 'iraq', ',', 'a', 'war', 'she', \"didn't\", 'agree', 'with', '.']\n",
      "\n",
      "['so', 'she', 'volunteered', 'to', 'serve', 'in', 'iraq_$$0', ',', 'a', 'war', 'she', \"didn't\", 'agree', 'with', '.']\n",
      "['so', 'she', 'volunteered', 'to', 'serve', 'in', 'iraq_$$1', ',', 'a', 'war', 'she', \"didn't\", 'agree', 'with', '.']\n",
      "\n",
      "['so', 'she', 'volunteered', 'to', 'serve', 'in', 'iraq', ',', 'a', 'war', 'she_$$0', \"didn't\", 'agree', 'with', '.']\n",
      "['so', 'she', 'volunteered', 'to', 'serve', 'in', 'iraq', ',', 'a', 'war', 'she_$$1', \"didn't\", 'agree', 'with', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_corpus = []\n",
    "\n",
    "#iterate over sentences in the corpus of tokenized sentences\n",
    "for sent in corpus:\n",
    "    word_changed = False\n",
    "    sent = list(sent)\n",
    "\n",
    "    # check each token by index and create a deep copy with the changed word at that index and add new sentence to new corpus\n",
    "    for idx,token in enumerate(sent):\n",
    "        if token in tokens_to_replace:\n",
    "            new_sent=copy.deepcopy(sent)\n",
    "            new_sent[idx] = f\"{token}_$$0\"\n",
    "            print(new_sent)\n",
    "            new_corpus.append(new_sent)\n",
    "            word_changed = True\n",
    "            \n",
    "            # depending on probability, add another copy of the new sentence with the second replacement synonym\n",
    "            if random.random() <= prob:\n",
    "                added_sent=copy.deepcopy(sent)\n",
    "                added_sent[idx] = f\"{token}_$$1\"\n",
    "                new_corpus.append(new_sent)\n",
    "                print(added_sent)\n",
    "                \n",
    "            print()\n",
    "    \n",
    "    # if no words were changed, just add the original sentence to the new corpus\n",
    "    if not word_changed:\n",
    "        new_corpus.append(sent)\n",
    "        print(sent)\n",
    "        print()\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "    \n",
    "    \"\"\"\n",
    "    # for each word to be replaced, see if it exists, if so, change it and add sentence to new corpus\n",
    "    for token in tokens_to_replace:\n",
    "        if token in sent:\n",
    "            new_sent = [word if word != token else f\"{word}_$$0\" for word in sent]\n",
    "            print(new_sent)\n",
    "            word_changed=True\n",
    "            new_corpus.append(new_sent)\n",
    "            \n",
    "            # check probability to see is an additional new copy should also be added with a new synonym\n",
    "            if random.random() <= prob:\n",
    "                added_sent = [word if \"_$$0\" not in word else word.replace(\"_$$0\", \"_$$1\") for word in new_sent]\n",
    "                new_corpus.append(added_sent)\n",
    "                print(added_sent)\n",
    "                \n",
    "    # if no words were changed in the sentence, just add the original sentence to the new corpus    \n",
    "    if not word_changed:\n",
    "        new_corpus.append(sent)\n",
    "        print(sent)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    #reconstruct the sentence, replacing any of the tokens_to_replace\n",
    "    for token in sent:\n",
    "        if token not in tokens_to_replace:\n",
    "            new_sent.append(token)\n",
    "        else:\n",
    "            new_sent.append(f\"{token}_$$0\")\n",
    "            possible_add = True  # flag indicating to check whether to append an extra new sentence to the corpus\n",
    "            \n",
    "    new_corpus.append(new_sent)\n",
    "    \n",
    "    #if the sentence had contained a replaceable token, check probability to append an additional new sentence\n",
    "    if possible_add:\n",
    "        if random.random() <= prob:\n",
    "            added_sent = [token if \"_$$0\" not in token else token.replace(\"_$$0\", \"_$$1\") for token in new_sent]\n",
    "            new_corpus.append(added_sent)\n",
    "\n",
    "   \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:2020-summer-surge]",
   "language": "python",
   "name": "conda-env-2020-summer-surge-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
